{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a680de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "input_folder_path = Path(\"data/raw\")\n",
    "train_path = \"training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "    # Reading the dataset with no columns titles and with latin encoding \n",
    "df = pd.read_csv(train_path, sep = \",\", encoding='latin-1', header=None, error_bad_lines=False)\n",
    "\n",
    "# As the data has no column titles, we will add our own\n",
    "df.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4de19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        clean_tweet: a list of words containing the processed tweet\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    clean_tweet = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            #clean_tweet.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            clean_tweet.append(stem_word)\n",
    "    \n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df496eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 200000\n"
     ]
    }
   ],
   "source": [
    "df_pos = df[df['label'] == 4]\n",
    "df_neg = df[df['label'] == 0]\n",
    "    \n",
    "    # Only retaining 1/4th of our data from each output group\n",
    "    # Feel free to alter the dividing factor depending on your workspace\n",
    "    # 1/64 is a good place to start if you're unsure about your machine's power\n",
    "df_pos = df_pos.iloc[:int(len(df_pos)/4)]\n",
    "df_neg = df_neg.iloc[:int(len(df_neg)/4)]\n",
    "print(len(df_pos), len(df_neg))\n",
    "\n",
    "all_positive_tweets = df_pos.text.to_list()\n",
    "all_negative_tweets = df_neg.text.to_list()\n",
    "\n",
    "val_pos   = all_positive_tweets[40000:] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:40000]# generating training set for positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da6d66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ricard\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ricard\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\ricard\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\ricard\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ricard\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed4c6da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Ricard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ricard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples \n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eeead2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "val_pos   = all_positive_tweets[40000:80000] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:40000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[40000:80000] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:40000] # generating training set for nagative tweets\n",
    "\n",
    "# Delete all_positive_tweets and all_negative_tweets from memory\n",
    "del all_positive_tweets\n",
    "del all_negative_tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "\n",
    "# Build the vocabulary\n",
    "\n",
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0178ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, train_neg, val_pos, val_neg, train_x, val_x, train_y, val_y, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8377b42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'love': 3,\n",
       " 'u': 4,\n",
       " 'guy': 5,\n",
       " 'r': 6,\n",
       " 'best': 7,\n",
       " 'im': 8,\n",
       " 'meet': 9,\n",
       " 'one': 10,\n",
       " 'besti': 11,\n",
       " 'tonight': 12,\n",
       " 'cant': 13,\n",
       " 'wait': 14,\n",
       " 'girl': 15,\n",
       " 'talk': 16,\n",
       " 'thank': 17,\n",
       " 'twitter': 18,\n",
       " 'add': 19,\n",
       " 'sunisa': 20,\n",
       " 'got': 21,\n",
       " 'hin': 22,\n",
       " 'show': 23,\n",
       " 'dc': 24,\n",
       " 'area': 25,\n",
       " 'sweetheart': 26,\n",
       " 'sick': 27,\n",
       " 'realli': 28,\n",
       " 'cheap': 29,\n",
       " 'hurt': 30,\n",
       " 'much': 31,\n",
       " 'eat': 32,\n",
       " 'real': 33,\n",
       " 'food': 34,\n",
       " 'plu': 35,\n",
       " 'friend': 36,\n",
       " 'make': 37,\n",
       " 'soup': 38,\n",
       " 'effect': 39,\n",
       " 'everyon': 40,\n",
       " 'tell': 41,\n",
       " 'burst': 42,\n",
       " 'laugh': 43,\n",
       " 'loud': 44,\n",
       " 'come': 45,\n",
       " 'sulk': 46,\n",
       " 'than': 47,\n",
       " 'respons': 48,\n",
       " 'ihad': 49,\n",
       " 'alreadi': 50,\n",
       " 'find': 51,\n",
       " 'answer': 52,\n",
       " 'jealou': 53,\n",
       " 'hope': 54,\n",
       " 'great': 55,\n",
       " 'time': 56,\n",
       " 'vega': 57,\n",
       " 'like': 58,\n",
       " \"acm'\": 59,\n",
       " 'ah': 60,\n",
       " 'congrat': 61,\n",
       " 'mr': 62,\n",
       " 'fletcher': 63,\n",
       " 'final': 64,\n",
       " 'join': 65,\n",
       " 'respond': 66,\n",
       " 'stupid': 67,\n",
       " 'cat': 68,\n",
       " 'help': 69,\n",
       " 'type': 70,\n",
       " 'forgiv': 71,\n",
       " 'error': 72,\n",
       " 'crazi': 73,\n",
       " 'day': 74,\n",
       " 'school': 75,\n",
       " '10': 76,\n",
       " 'hour': 77,\n",
       " 'straiiight': 78,\n",
       " 'watch': 79,\n",
       " 'hill': 80,\n",
       " 'told': 81,\n",
       " 'ha': 82,\n",
       " 'happi': 83,\n",
       " 'birthday': 84,\n",
       " 'jb': 85,\n",
       " 'forget': 86,\n",
       " 'two': 87,\n",
       " 'half': 88,\n",
       " 'men': 89,\n",
       " 'haha': 90,\n",
       " 'worri': 91,\n",
       " 'get': 92,\n",
       " 'hang': 93,\n",
       " 'least': 94,\n",
       " 'feel': 95,\n",
       " 'lost': 96,\n",
       " 'may': 97,\n",
       " 'caus': 98,\n",
       " 'mani': 99,\n",
       " 'later': 100,\n",
       " 'usual': 101,\n",
       " 'night': 102,\n",
       " 'addict': 103,\n",
       " 'sure': 104,\n",
       " 'dm': 105,\n",
       " 'post': 106,\n",
       " 'link': 107,\n",
       " 'video': 108,\n",
       " '<lol>': 109,\n",
       " 'miss': 110,\n",
       " 'better': 111,\n",
       " 'permiss': 112,\n",
       " 'bless': 113,\n",
       " 'first': 114,\n",
       " 'ad': 115,\n",
       " 'tweeti': 116,\n",
       " 'new': 117,\n",
       " 'iphon': 118,\n",
       " 'know': 119,\n",
       " 'think': 120,\n",
       " 'globe': 121,\n",
       " 'yeah': 122,\n",
       " 'sana': 123,\n",
       " 'gumal': 124,\n",
       " 'na': 125,\n",
       " 'ko': 126,\n",
       " 'para': 127,\n",
       " 'alam': 128,\n",
       " 'din': 129,\n",
       " 'kung': 130,\n",
       " 'makakasama': 131,\n",
       " 'ako': 132,\n",
       " 'pictur': 133,\n",
       " 'sweet': 134,\n",
       " 'catch': 135,\n",
       " 'email': 136,\n",
       " 'rss': 137,\n",
       " 'random': 138,\n",
       " 'bacn': 139,\n",
       " \"i'm\": 140,\n",
       " 'cut': 141,\n",
       " 'earli': 142,\n",
       " '11:30': 143,\n",
       " 'pm': 144,\n",
       " 'dinner': 145,\n",
       " 'danc': 146,\n",
       " 'around': 147,\n",
       " 'room': 148,\n",
       " 'pj': 149,\n",
       " 'jam': 150,\n",
       " 'ipod': 151,\n",
       " 'dizzi': 152,\n",
       " 'well': 153,\n",
       " 'ask': 154,\n",
       " 'place': 155,\n",
       " 'peep': 156,\n",
       " 'contest': 157,\n",
       " 'vote': 158,\n",
       " 'anyway': 159,\n",
       " 'go': 160,\n",
       " 'bed': 161,\n",
       " 'goodnight': 162,\n",
       " 'dream': 163,\n",
       " 'walk': 164,\n",
       " 'put': 165,\n",
       " 'deposit': 166,\n",
       " 'tomorrow': 167,\n",
       " 'followinq': 168,\n",
       " 'shud': 169,\n",
       " 'tha': 170,\n",
       " 'meant': 171,\n",
       " 'back': 172,\n",
       " 'toilet': 173,\n",
       " 'cubicl': 174,\n",
       " 'somewher': 175,\n",
       " 'aw': 176,\n",
       " 'hold': 177,\n",
       " 'puppi': 178,\n",
       " \"he'\": 179,\n",
       " 'mine': 180,\n",
       " 'cuti': 181,\n",
       " 'kitteh': 182,\n",
       " 'sleepin': 183,\n",
       " 'crotch': 184,\n",
       " 'prove': 185,\n",
       " 'agre': 186,\n",
       " 'reach': 187,\n",
       " 'amritsar': 188,\n",
       " 'bu': 189,\n",
       " 'wagah': 190,\n",
       " 'border': 191,\n",
       " '2pm': 192,\n",
       " 'let': 193,\n",
       " 'interview': 194,\n",
       " 'famou': 195,\n",
       " 'super': 196,\n",
       " 'star': 197,\n",
       " 'spend': 198,\n",
       " 'famili': 199,\n",
       " 'tire': 200,\n",
       " 'gonna': 201,\n",
       " 'didnt': 202,\n",
       " '=/': 203,\n",
       " 'ef': 204,\n",
       " 'throat': 205,\n",
       " '...': 206,\n",
       " 'ooohh': 207,\n",
       " 'crave': 208,\n",
       " 'pina': 209,\n",
       " 'colada': 210,\n",
       " 'banana': 211,\n",
       " 'slushi': 212,\n",
       " 'upload': 213,\n",
       " 'di': 214,\n",
       " 'indowebst': 215,\n",
       " 'dong': 216,\n",
       " 'banggg': 217,\n",
       " 'welcom': 218,\n",
       " 'glad': 219,\n",
       " 'enjoy': 220,\n",
       " 'hahaha': 221,\n",
       " 'omg': 222,\n",
       " 'wer': 223,\n",
       " 'laughin': 224,\n",
       " 'hook': 225,\n",
       " 'cuz': 226,\n",
       " 'da': 227,\n",
       " 'rolll': 228,\n",
       " 'ryt': 229,\n",
       " 'sheldawg': 230,\n",
       " 'awww': 231,\n",
       " 'pooki': 232,\n",
       " \"i'll\": 233,\n",
       " 'pray': 234,\n",
       " 'bag': 235,\n",
       " 'nurs': 236,\n",
       " 'yay': 237,\n",
       " 'found': 238,\n",
       " 'cuddl': 239,\n",
       " 'buddi': 240,\n",
       " 'met': 241,\n",
       " 'snob': 242,\n",
       " 'bad': 243,\n",
       " 'life': 244,\n",
       " 'goe': 245,\n",
       " 'across': 246,\n",
       " 'univers': 247,\n",
       " 'sleep': 248,\n",
       " 'rehears': 249,\n",
       " 'dri': 250,\n",
       " 'potato': 251,\n",
       " 'huh': 252,\n",
       " 'hi': 253,\n",
       " 'jon': 254,\n",
       " 'hear': 255,\n",
       " 'see': 256,\n",
       " 'cruis': 257,\n",
       " 'cannot': 258,\n",
       " 'knight': 259,\n",
       " 'long': 260,\n",
       " 'convers': 261,\n",
       " 'mom': 262,\n",
       " 'phone': 263,\n",
       " 'yayyy': 264,\n",
       " 'lol': 265,\n",
       " 'request': 266,\n",
       " 'herrr': 267,\n",
       " 'say': 268,\n",
       " 'practic': 269,\n",
       " 'line': 270,\n",
       " 'manner': 271,\n",
       " 'voic': 272,\n",
       " 'upcom': 273,\n",
       " 'featur': 274,\n",
       " 'shoot': 275,\n",
       " 'probabl': 276,\n",
       " 'drive': 277,\n",
       " 'brock': 278,\n",
       " 'read': 279,\n",
       " 'moon': 280,\n",
       " '3rd': 281,\n",
       " 'enough': 282,\n",
       " 'twilight': 283,\n",
       " 'seri': 284,\n",
       " 'today': 285,\n",
       " 'month': 286,\n",
       " 'anniversari': 287,\n",
       " 'sooo': 288,\n",
       " 'diana': 289,\n",
       " 'dont': 290,\n",
       " 'even': 291,\n",
       " 'tar': 292,\n",
       " 'heel': 293,\n",
       " 'ncaa': 294,\n",
       " 'woot': 295,\n",
       " 'bore': 296,\n",
       " 'wallpap': 297,\n",
       " 'red': 298,\n",
       " 'squar': 299,\n",
       " 'notic': 300,\n",
       " 'bb': 301,\n",
       " 'tryna': 302,\n",
       " 'inspir': 303,\n",
       " 'that': 304,\n",
       " \"can't\": 305,\n",
       " 'workout': 306,\n",
       " 'ur': 307,\n",
       " 'hot': 308,\n",
       " 'ass': 309,\n",
       " 'austin': 310,\n",
       " 'annnd': 311,\n",
       " 'bob': 312,\n",
       " 'purpl': 313,\n",
       " 'went': 314,\n",
       " 'royal': 315,\n",
       " 'color': 316,\n",
       " 'way': 317,\n",
       " 'bunch': 318,\n",
       " 'buffi': 319,\n",
       " 'song': 320,\n",
       " 'fave': 321,\n",
       " 'vivian': 322,\n",
       " 'nerf': 323,\n",
       " 'herder': 324,\n",
       " 'faith': 325,\n",
       " 'spike': 326,\n",
       " \"buffy'\": 327,\n",
       " 'bodi': 328,\n",
       " 'devun': 329,\n",
       " 'check': 330,\n",
       " '-->': 331,\n",
       " 'morn': 332,\n",
       " 'tweetland': 333,\n",
       " 'ahead': 334,\n",
       " 'friendster': 335,\n",
       " 'pic': 336,\n",
       " ';-p': 337,\n",
       " 'noodlebox': 338,\n",
       " 'amanda': 339,\n",
       " 'sorrri': 340,\n",
       " '2982378': 341,\n",
       " '2': 342,\n",
       " 'differ': 343,\n",
       " 'thing': 344,\n",
       " 'kevin': 345,\n",
       " 'jona': 346,\n",
       " 'girlfriend': 347,\n",
       " 'seriou': 348,\n",
       " 'sing': 349,\n",
       " '..': 350,\n",
       " 'whoa': 351,\n",
       " 'hehe': 352,\n",
       " 'lsd': 353,\n",
       " 'mayb': 354,\n",
       " 'j': 355,\n",
       " 'k': 356,\n",
       " 'lolol': 357,\n",
       " 'sicilian': 358,\n",
       " 'damn': 359,\n",
       " 'pizza': 360,\n",
       " 'planet': 361,\n",
       " 'chocol': 362,\n",
       " 'milk': 363,\n",
       " 'gf': 364,\n",
       " 'sutter': 365,\n",
       " 'creek': 366,\n",
       " 'tour': 367,\n",
       " 'old': 368,\n",
       " 'look': 369,\n",
       " 'cho': 370,\n",
       " 'person': 371,\n",
       " 'strawberri': 372,\n",
       " 'becom': 373,\n",
       " 'coach': 374,\n",
       " 'soon': 375,\n",
       " 'guess': 376,\n",
       " 'live': 377,\n",
       " 'side': 378,\n",
       " 'earth': 379,\n",
       " 'east': 380,\n",
       " 'mesa': 381,\n",
       " 'losiento': 382,\n",
       " 'apart': 383,\n",
       " 'sore': 384,\n",
       " 'tryout': 385,\n",
       " 'fail': 386,\n",
       " 'physic': 387,\n",
       " 'test': 388,\n",
       " 'homo': 389,\n",
       " 'keeen': 390,\n",
       " 'holiday': 391,\n",
       " 'observ': 392,\n",
       " 'updat': 393,\n",
       " 'alexa': 394,\n",
       " 'websit': 395,\n",
       " 'us': 396,\n",
       " 'total': 397,\n",
       " 'geek': 398,\n",
       " 'right': 399,\n",
       " 'bonjour': 400,\n",
       " 'spacecowboy': 401,\n",
       " 'wish': 402,\n",
       " 'either': 403,\n",
       " 'work': 404,\n",
       " 'kid': 405,\n",
       " 'whenev': 406,\n",
       " 'want': 407,\n",
       " 'wherev': 408,\n",
       " 'brother': 409,\n",
       " 'quit': 410,\n",
       " 'weird': 411,\n",
       " 'give': 412,\n",
       " 'hug': 413,\n",
       " 'cooki': 414,\n",
       " 'hey': 415,\n",
       " 'take': 416,\n",
       " 'care': 417,\n",
       " 'rundown': 418,\n",
       " 'wrote': 419,\n",
       " 'wall': 420,\n",
       " 'correct': 421,\n",
       " 'poor': 422,\n",
       " 'spell': 423,\n",
       " 'highlight': 424,\n",
       " 'wash': 425,\n",
       " 'hardcor': 426,\n",
       " 'good': 427,\n",
       " 'tech': 428,\n",
       " 'clubzon': 429,\n",
       " 'sushi': 430,\n",
       " 'suggest': 431,\n",
       " 'bro': 432,\n",
       " 'rock': 433,\n",
       " 'depend': 434,\n",
       " 'version': 435,\n",
       " 'thought': 436,\n",
       " ';)': 437,\n",
       " 'hand': 438,\n",
       " 'still': 439,\n",
       " 'pretti': 440,\n",
       " 'weak': 441,\n",
       " 'punch': 442,\n",
       " 'anyon': 443,\n",
       " 'yet': 444,\n",
       " 'turn': 445,\n",
       " 'knob': 446,\n",
       " 'doesnt': 447,\n",
       " 'lurker': 448,\n",
       " 'interest': 449,\n",
       " 'b': 450,\n",
       " 'north': 451,\n",
       " 'need': 452,\n",
       " 'decid': 453,\n",
       " 'aaru': 454,\n",
       " 'hithavaru': 455,\n",
       " 'ninag': 456,\n",
       " 'ee': 457,\n",
       " 'moovarolag': 458,\n",
       " 'um': 459,\n",
       " 'bought': 460,\n",
       " 'shit': 461,\n",
       " 'korean': 462,\n",
       " 'oach': 463,\n",
       " 'highest': 464,\n",
       " 'qualiti': 465,\n",
       " 'babi': 466,\n",
       " 'paid': 467,\n",
       " 'extra': 468,\n",
       " '3.50': 469,\n",
       " 'tag': 470,\n",
       " 'sew': 471,\n",
       " 'ice-cream': 472,\n",
       " 'top': 473,\n",
       " 'finish': 474,\n",
       " '200': 475,\n",
       " 'crunch': 476,\n",
       " 'step': 477,\n",
       " 'diddi': 478,\n",
       " 'daaang': 479,\n",
       " 'possibl': 480,\n",
       " \"what'd\": 481,\n",
       " 'west': 482,\n",
       " 'mostli': 483,\n",
       " 'san': 484,\n",
       " 'jose': 485,\n",
       " 'francisco': 486,\n",
       " 'nikki': 487,\n",
       " 'bike': 488,\n",
       " 'tri': 489,\n",
       " 'recov': 490,\n",
       " 'knee': 491,\n",
       " 'injuri': 492,\n",
       " 'would': 493,\n",
       " 'unpack': 494,\n",
       " 'sat': 495,\n",
       " 'afternoon': 496,\n",
       " 'shenagian': 497,\n",
       " 'team': 498,\n",
       " 'awesom': 499,\n",
       " '<3': 500,\n",
       " 'listen': 501,\n",
       " 'music': 502,\n",
       " 'chill': 503,\n",
       " 'regret': 504,\n",
       " 'done': 505,\n",
       " 'till': 506,\n",
       " 'kick': 507,\n",
       " 'studi': 508,\n",
       " 'grow': 509,\n",
       " 'rich': 510,\n",
       " 'wow': 511,\n",
       " 'amaz': 512,\n",
       " 'timeless': 513,\n",
       " 'law': 514,\n",
       " 'attract': 515,\n",
       " \"we'r\": 516,\n",
       " '6': 517,\n",
       " 'program': 518,\n",
       " 'oh': 519,\n",
       " \"i'v\": 520,\n",
       " \"it'll\": 521,\n",
       " 'week': 522,\n",
       " 'though': 523,\n",
       " '21': 524,\n",
       " 'chri': 525,\n",
       " 'home': 526,\n",
       " '67': 527,\n",
       " 'summer': 528,\n",
       " '1.47': 529,\n",
       " 'ish': 530,\n",
       " 'tweet': 531,\n",
       " 'drink': 532,\n",
       " 'kay': 533,\n",
       " 'determin': 534,\n",
       " 'basebal': 535,\n",
       " '5': 536,\n",
       " 'year': 537,\n",
       " 'sinc': 538,\n",
       " 'sport': 539,\n",
       " 'game': 540,\n",
       " 'becca': 541,\n",
       " 'ohhh': 542,\n",
       " 'nkow': 543,\n",
       " 'lexington': 544,\n",
       " 'ky': 545,\n",
       " 'rite': 546,\n",
       " 'next': 547,\n",
       " 'seen': 548,\n",
       " 'preview': 549,\n",
       " 'movi': 550,\n",
       " 'obsess': 551,\n",
       " 'w': 552,\n",
       " 'beyonc': 553,\n",
       " 'broad': 554,\n",
       " 'seem': 555,\n",
       " 'nut': 556,\n",
       " 'easi': 557,\n",
       " 'rider': 558,\n",
       " 'fall': 559,\n",
       " 'asleep': 560,\n",
       " 'excit': 561,\n",
       " 'broadcast': 562,\n",
       " 'paris-roubaix': 563,\n",
       " 'moment': 564,\n",
       " 'misss': 565,\n",
       " 'peopl': 566,\n",
       " 'surviv': 567,\n",
       " 'winter': 568,\n",
       " 'without': 569,\n",
       " 'jungl': 570,\n",
       " 'gym': 571,\n",
       " 'start': 572,\n",
       " 'last': 573,\n",
       " 'holidayholiday': 574,\n",
       " 'watchin': 575,\n",
       " 'greek': 576,\n",
       " 'knighti': 577,\n",
       " 'media': 578,\n",
       " 'design': 579,\n",
       " 'client': 580,\n",
       " 'profil': 581,\n",
       " 'lame': 582,\n",
       " 'stalk': 583,\n",
       " 'sexi': 584,\n",
       " 'aww': 585,\n",
       " 'anoth': 586,\n",
       " 'nitey': 587,\n",
       " 'nite': 588,\n",
       " 'ahh': 589,\n",
       " 'kinda': 590,\n",
       " 'nervou': 591,\n",
       " 'til': 592,\n",
       " 'plan': 593,\n",
       " 'whatev': 594,\n",
       " 'happier': 595,\n",
       " \"here'\": 596,\n",
       " 'hopin': 597,\n",
       " 'fb': 598,\n",
       " 'store': 599,\n",
       " 'mean': 600,\n",
       " 'pam': 601,\n",
       " 'ye': 602,\n",
       " 'short': 603,\n",
       " 'struggl': 604,\n",
       " 'choic': 605,\n",
       " 'everi': 606,\n",
       " 'hmm': 607,\n",
       " 'lot': 608,\n",
       " \"that'\": 609,\n",
       " 'cours': 610,\n",
       " 'late': 611,\n",
       " '100,000': 612,\n",
       " 'follow': 613,\n",
       " 'almost': 614,\n",
       " '4': 615,\n",
       " 'nm': 616,\n",
       " 'scream': 617,\n",
       " 'wonder': 618,\n",
       " 'freakin': 619,\n",
       " 'coool': 620,\n",
       " \"there'\": 621,\n",
       " 'noth': 622,\n",
       " 'wrong': 623,\n",
       " 'brissa': 624,\n",
       " 'lauren': 625,\n",
       " 'stori': 626,\n",
       " 'mirror': 627,\n",
       " 'neat': 628,\n",
       " 'god': 629,\n",
       " 'overitnow': 630,\n",
       " 'apolog': 631,\n",
       " 'share': 632,\n",
       " 'spiritu': 633,\n",
       " 'gener': 634,\n",
       " 'public': 635,\n",
       " '31': 636,\n",
       " 'leav': 637,\n",
       " 'ontario': 638,\n",
       " 'spoke': 639,\n",
       " 'fam': 640,\n",
       " 'japan': 641,\n",
       " 'via': 642,\n",
       " 'skype': 643,\n",
       " 'littl': 644,\n",
       " '16': 645,\n",
       " 'mo-old': 646,\n",
       " 'nephew': 647,\n",
       " 'saw': 648,\n",
       " 'written': 649,\n",
       " 'someon': 650,\n",
       " 'spoken': 651,\n",
       " 'suck': 652,\n",
       " 'mcfli': 653,\n",
       " 'loser': 654,\n",
       " 'holi': 655,\n",
       " 'laundri': 656,\n",
       " 'woman': 657,\n",
       " 'hate': 658,\n",
       " 'trainer': 659,\n",
       " 'youï': 660,\n",
       " '¿': 661,\n",
       " '½': 662,\n",
       " 'fuck': 663,\n",
       " 'funni': 664,\n",
       " 'set': 665,\n",
       " 'touch': 666,\n",
       " 'offici': 667,\n",
       " 'tvc': 668,\n",
       " 'yesterday': 669,\n",
       " 'could': 670,\n",
       " 'manni': 671,\n",
       " 'peddi': 672,\n",
       " 'shut': 673,\n",
       " 'fat': 674,\n",
       " 'face': 675,\n",
       " 'ever': 676,\n",
       " 'run': 677,\n",
       " 'idea': 678,\n",
       " 'nomin': 679,\n",
       " 'beer': 680,\n",
       " 'monday': 681,\n",
       " 'never': 682,\n",
       " 'alway': 683,\n",
       " 'special': 684,\n",
       " '72': 685,\n",
       " 'luck': 686,\n",
       " 'rest': 687,\n",
       " 'cast': 688,\n",
       " 'tb': 689,\n",
       " 'someth': 690,\n",
       " 'leak': 691,\n",
       " 'land': 692,\n",
       " 'among': 693,\n",
       " 'entir': 694,\n",
       " 'creativ': 695,\n",
       " 'vibrant': 696,\n",
       " 'reason': 697,\n",
       " 'nada': 698,\n",
       " 'holla': 699,\n",
       " 'how': 700,\n",
       " 'mixtap': 701,\n",
       " 'yep': 702,\n",
       " 'easier': 703,\n",
       " 'point': 704,\n",
       " '140': 705,\n",
       " 'freezer-worthi': 706,\n",
       " 'given': 707,\n",
       " 'scholarli': 708,\n",
       " 'attent': 709,\n",
       " 'suppos': 710,\n",
       " 'eye': 711,\n",
       " 'favor': 712,\n",
       " 'use': 713,\n",
       " 'english': 714,\n",
       " 'languag': 715,\n",
       " 'explet': 716,\n",
       " 'flippin': 717,\n",
       " 'cold': 718,\n",
       " 'outsid': 719,\n",
       " 'ago': 720,\n",
       " 'sunni': 721,\n",
       " 'rain': 722,\n",
       " 'snow': 723,\n",
       " 'ate': 724,\n",
       " 'raw': 725,\n",
       " 'dough': 726,\n",
       " 'tub': 727,\n",
       " 'must': 728,\n",
       " 'kindr': 729,\n",
       " 'spirit': 730,\n",
       " 'internet': 731,\n",
       " 'dirti': 732,\n",
       " 'nighti': 733,\n",
       " 'tweetdeck': 734,\n",
       " 'facebook': 735,\n",
       " 'dude': 736,\n",
       " 'surpris': 737,\n",
       " 'figur': 738,\n",
       " 'llama': 739,\n",
       " 'malcolm': 740,\n",
       " 'megan': 741,\n",
       " '<---': 742,\n",
       " 'theatr': 743,\n",
       " 'martini': 744,\n",
       " 'citi': 745,\n",
       " 'soo': 746,\n",
       " 'spring': 747,\n",
       " 'break': 748,\n",
       " 'moviess': 749,\n",
       " 'everyday': 750,\n",
       " 'grandpa': 751,\n",
       " 'ice': 752,\n",
       " 'chest': 753,\n",
       " 'beef': 754,\n",
       " 'georg': 755,\n",
       " 'lopez': 756,\n",
       " 'rulesss': 757,\n",
       " 'kati': 758,\n",
       " 'head': 759,\n",
       " 'club': 760,\n",
       " 'speak': 761,\n",
       " 'stellar': 762,\n",
       " 'homey': 763,\n",
       " 'shhh': 764,\n",
       " 'photo': 765,\n",
       " 'hit': 766,\n",
       " 'count': 767,\n",
       " 'china': 768,\n",
       " '5,000': 769,\n",
       " 'view': 770,\n",
       " 'alon': 771,\n",
       " 'came': 772,\n",
       " 'appreci': 773,\n",
       " 'everywher': 774,\n",
       " 'provid': 775,\n",
       " 'broadband': 776,\n",
       " 'kiddin': 777,\n",
       " 'callin': 778,\n",
       " 'hash': 779,\n",
       " 'bash': 780,\n",
       " 'badass': 781,\n",
       " 'fring': 782,\n",
       " 'hmmm': 783,\n",
       " 'upto': 784,\n",
       " 'christma': 785,\n",
       " 'bn': 786,\n",
       " 'fun': 787,\n",
       " 'caught': 788,\n",
       " 'yrold': 789,\n",
       " 'cheat': 790,\n",
       " 'reflect': 791,\n",
       " 'tv': 792,\n",
       " 'hair': 793,\n",
       " 'blue': 794,\n",
       " 'kind': 795,\n",
       " 'word': 796,\n",
       " 'stress': 797,\n",
       " 'math': 798,\n",
       " 'uhhh': 799,\n",
       " 'uh': 800,\n",
       " 'driver': 801,\n",
       " 'pch': 802,\n",
       " 'whatcha': 803,\n",
       " 'chucklin': 804,\n",
       " 'bout': 805,\n",
       " 'ms.yen': 806,\n",
       " 'climb': 807,\n",
       " 'sine': 808,\n",
       " '20': 809,\n",
       " 'unc': 810,\n",
       " 'prepar': 811,\n",
       " \"g'night\": 812,\n",
       " 'includ': 813,\n",
       " 'magic': 814,\n",
       " 'fountain': 815,\n",
       " 'trip': 816,\n",
       " 'trusti': 817,\n",
       " 'book': 818,\n",
       " 'anastasia': 819,\n",
       " 'laptop': 820,\n",
       " 'buzz': 821,\n",
       " 'sietar': 822,\n",
       " 'push': 823,\n",
       " 'boundari': 824,\n",
       " 'boy': 825,\n",
       " 'money': 826,\n",
       " 'manual': 827,\n",
       " 'terribl': 828,\n",
       " 'neck': 829,\n",
       " 'ach': 830,\n",
       " 'chair': 831,\n",
       " 'swopper': 832,\n",
       " 'arriv': 833,\n",
       " 'hellz': 834,\n",
       " 'definit': 835,\n",
       " 'download': 836,\n",
       " 'hahah': 837,\n",
       " 'blog': 838,\n",
       " 'nice': 839,\n",
       " 'recent': 840,\n",
       " 'popular': 841,\n",
       " 'list': 842,\n",
       " 'togeth': 843,\n",
       " 'common': 844,\n",
       " 'wanna': 845,\n",
       " 'save': 846,\n",
       " 'dog': 847,\n",
       " 'anim': 848,\n",
       " 'call': 849,\n",
       " 'twit': 850,\n",
       " 'polic': 851,\n",
       " 'nilagyan': 852,\n",
       " 'mo': 853,\n",
       " 'ng': 854,\n",
       " 'knorr': 855,\n",
       " 'season': 856,\n",
       " 'mix': 857,\n",
       " 'ulit': 858,\n",
       " 'ung': 859,\n",
       " 'pancak': 860,\n",
       " 'heheh': 861,\n",
       " 'teka': 862,\n",
       " 'heaven': 863,\n",
       " 'egg': 864,\n",
       " 'ba': 865,\n",
       " 'hello': 866,\n",
       " 'westney': 867,\n",
       " 'x': 868,\n",
       " 'comparison': 869,\n",
       " 'stroke': 870,\n",
       " 'experienc': 871,\n",
       " 'draw': 872,\n",
       " 'giraff': 873,\n",
       " 'eyelash': 874,\n",
       " 'ahhh': 875,\n",
       " 'shiner': 876,\n",
       " 'bock': 877,\n",
       " 'reliv': 878,\n",
       " 'spent': 879,\n",
       " 'beauti': 880,\n",
       " 'warm': 881,\n",
       " 'weather': 882,\n",
       " 'pullin': 883,\n",
       " 'nighter': 884,\n",
       " 'wit': 885,\n",
       " 'bffl': 886,\n",
       " 'madison': 887,\n",
       " 'event': 888,\n",
       " 'inevit': 889,\n",
       " 'sequel': 890,\n",
       " 'hilari': 891,\n",
       " 'row': 892,\n",
       " 'dooo': 893,\n",
       " 'big': 894,\n",
       " 'oaf': 895,\n",
       " 'play': 896,\n",
       " 'piano': 897,\n",
       " 'bryon': 898,\n",
       " 'tea': 899,\n",
       " 'bitch': 900,\n",
       " 'stay': 901,\n",
       " 'yup': 902,\n",
       " 'match': 903,\n",
       " 'offic': 904,\n",
       " 'harder': 905,\n",
       " 'peac': 906,\n",
       " 'prayer': 907,\n",
       " 'ya': 908,\n",
       " 'beat': 909,\n",
       " 'traffic': 910,\n",
       " 'india': 911,\n",
       " 'win': 912,\n",
       " 'zealand': 913,\n",
       " '41': 914,\n",
       " 'wasnt': 915,\n",
       " 'third': 916,\n",
       " 'madra': 917,\n",
       " 'road': 918,\n",
       " 'concept': 919,\n",
       " 'park': 920,\n",
       " 'appli': 921,\n",
       " 'class': 922,\n",
       " 'manga': 923,\n",
       " 'bring': 924,\n",
       " 'joy': 925,\n",
       " 'roflmao': 926,\n",
       " 'niqqa': 927,\n",
       " 'pleas': 928,\n",
       " 'ackin': 929,\n",
       " 'fool': 930,\n",
       " 'also': 931,\n",
       " 'fyi': 932,\n",
       " 'piti': 933,\n",
       " 'knew': 934,\n",
       " \"let'\": 935,\n",
       " 'profession': 936,\n",
       " 'octob': 937,\n",
       " '2008': 938,\n",
       " \"we'v\": 939,\n",
       " 'anyth': 940,\n",
       " 'tip': 941,\n",
       " \"sam'\": 942,\n",
       " 'nighttim': 943,\n",
       " 'twin': 944,\n",
       " 'ember': 945,\n",
       " 'xd': 946,\n",
       " 'die': 947,\n",
       " 'actual': 948,\n",
       " 'keep': 949,\n",
       " ':d': 950,\n",
       " 'rive': 951,\n",
       " 'mate': 952,\n",
       " 'darn': 953,\n",
       " \"neil'\": 954,\n",
       " 'nick': 955,\n",
       " \"nora'\": 956,\n",
       " 'playlist': 957,\n",
       " 'perfect': 958,\n",
       " 'venu': 959,\n",
       " 'whine': 960,\n",
       " 'wast': 961,\n",
       " 'hell': 962,\n",
       " 'effin': 963,\n",
       " 'man': 964,\n",
       " 'lazi': 965,\n",
       " 'tool': 966,\n",
       " 'messag': 967,\n",
       " 'ai': 968,\n",
       " 'buy': 969,\n",
       " 'tina': 970,\n",
       " \"parol'\": 971,\n",
       " \"who'\": 972,\n",
       " 'itun': 973,\n",
       " 'debbi': 974,\n",
       " 'sixti': 975,\n",
       " 'six': 976,\n",
       " 'bowl': 977,\n",
       " 'dear': 978,\n",
       " 'dbq': 979,\n",
       " 'pain': 980,\n",
       " 'asss': 981,\n",
       " 'chat': 982,\n",
       " 'coincid': 983,\n",
       " 'feed': 984,\n",
       " 'gsoc': 985,\n",
       " 'news': 986,\n",
       " 'discov': 987,\n",
       " 'easter': 988,\n",
       " 'craft': 989,\n",
       " 'cross': 990,\n",
       " 'bun': 991,\n",
       " 'princ': 992,\n",
       " 'egypt': 993,\n",
       " 'aid': 994,\n",
       " 'medit': 995,\n",
       " 'nap': 996,\n",
       " 'twitterpeep': 997,\n",
       " 'pay': 998,\n",
       " 'forward': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8dccd82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 200000\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\x9a' in position 1: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-a0b424975444>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mtemp_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_path\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\x9a' in position 1: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "input_folder_path = Path(\"data/raw\")\n",
    "train_path = \"training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "    # Reading the dataset with no columns titles and with latin encoding \n",
    "df = pd.read_csv(train_path, sep = \",\", encoding='latin-1', header=None, error_bad_lines=False)\n",
    "\n",
    "# As the data has no column titles, we will add our own\n",
    "df.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "\n",
    "# Separating positive and negative rows\n",
    "df_pos = df[df['label'] == 4]\n",
    "df_neg = df[df['label'] == 0]\n",
    "    \n",
    "    # Only retaining 1/4th of our data from each output group\n",
    "    # Feel free to alter the dividing factor depending on your workspace\n",
    "    # 1/64 is a good place to start if you're unsure about your machine's power\n",
    "df_pos = df_pos.iloc[:int(len(df_pos)/4)]\n",
    "df_neg = df_neg.iloc[:int(len(df_neg)/4)]\n",
    "print(len(df_pos), len(df_neg))\n",
    "\n",
    "all_positive_tweets = df_pos.text.to_list()\n",
    "all_negative_tweets = df_neg.text.to_list()\n",
    "\n",
    "val_pos   = all_positive_tweets[40000:80000] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:40000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[40000:80000] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:40000] # generating training set for nagative tweets\n",
    "\n",
    "# Delete all_positive_tweets and all_negative_tweets from memory\n",
    "del all_positive_tweets\n",
    "del all_negative_tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "\n",
    "# Build the vocabulary\n",
    "\n",
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "\n",
    "# Path of the output data folder\n",
    "Path(\"data/processed\").mkdir(exist_ok=True)\n",
    "prepared_folder_path = Path(\"data/processed\")\n",
    "\n",
    "X_train_path = prepared_folder_path / \"X_train.txt\"\n",
    "y_train_path = prepared_folder_path / \"y_train.txt\"\n",
    "X_valid_path = prepared_folder_path / \"X_valid.txt\"\n",
    "y_valid_path = prepared_folder_path / \"y_valid.txt\"\n",
    "train_pos_path = prepared_folder_path / \"train_pos.txt\"\n",
    "train_neg_path = prepared_folder_path / \"train_neg.txt\"\n",
    "vocab_path = prepared_folder_path / \"vocab.json\"\n",
    "\n",
    "with open(X_train_path, 'w') as temp_file:\n",
    "    for item in train_x:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(y_train_path , 'w') as temp_file:\n",
    "    for item in train_y:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(X_valid_path, 'w') as temp_file:\n",
    "    for item in val_x:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(y_valid_path, 'w') as temp_file:\n",
    "    for item in val_y:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open(train_pos_path, 'w') as temp_file:\n",
    "    for item in train_pos:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(train_neg_path, 'w') as temp_file:\n",
    "    for item in train_neg:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(vocab_path, 'wb') as fp:\n",
    "    pickle.dump(Vocab, fp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2da902c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "binary mode doesn't take an encoding argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d4bcec14b7ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtemp_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: binary mode doesn't take an encoding argument"
     ]
    }
   ],
   "source": [
    "with open(X_train_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in train_x:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(y_train_path , 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in train_y:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(X_valid_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in val_x:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(y_valid_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in val_y:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open(train_pos_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in train_pos:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(train_neg_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in train_neg:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "        \n",
    "import json\n",
    "with open(vocab_path, 'w', encoding = 'utf-8') as fp:\n",
    "    json.dump(Vocab, fp)  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3859f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(vocab_path, 'w', encoding = 'utf-8') as fp:\n",
    "    json.dump(Vocab, fp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "944a6831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'love': 3,\n",
       " 'u': 4,\n",
       " 'guy': 5,\n",
       " 'r': 6,\n",
       " 'best': 7,\n",
       " 'im': 8,\n",
       " 'meet': 9,\n",
       " 'one': 10,\n",
       " 'besti': 11,\n",
       " 'tonight': 12,\n",
       " 'cant': 13,\n",
       " 'wait': 14,\n",
       " 'girl': 15,\n",
       " 'talk': 16,\n",
       " 'thank': 17,\n",
       " 'twitter': 18,\n",
       " 'add': 19,\n",
       " 'sunisa': 20,\n",
       " 'got': 21,\n",
       " 'hin': 22,\n",
       " 'show': 23,\n",
       " 'dc': 24,\n",
       " 'area': 25,\n",
       " 'sweetheart': 26,\n",
       " 'sick': 27,\n",
       " 'realli': 28,\n",
       " 'cheap': 29,\n",
       " 'hurt': 30,\n",
       " 'much': 31,\n",
       " 'eat': 32,\n",
       " 'real': 33,\n",
       " 'food': 34,\n",
       " 'plu': 35,\n",
       " 'friend': 36,\n",
       " 'make': 37,\n",
       " 'soup': 38,\n",
       " 'effect': 39,\n",
       " 'everyon': 40,\n",
       " 'tell': 41,\n",
       " 'burst': 42,\n",
       " 'laugh': 43,\n",
       " 'loud': 44,\n",
       " 'come': 45,\n",
       " 'sulk': 46,\n",
       " 'than': 47,\n",
       " 'respons': 48,\n",
       " 'ihad': 49,\n",
       " 'alreadi': 50,\n",
       " 'find': 51,\n",
       " 'answer': 52,\n",
       " 'jealou': 53,\n",
       " 'hope': 54,\n",
       " 'great': 55,\n",
       " 'time': 56,\n",
       " 'vega': 57,\n",
       " 'like': 58,\n",
       " \"acm'\": 59,\n",
       " 'ah': 60,\n",
       " 'congrat': 61,\n",
       " 'mr': 62,\n",
       " 'fletcher': 63,\n",
       " 'final': 64,\n",
       " 'join': 65,\n",
       " 'respond': 66,\n",
       " 'stupid': 67,\n",
       " 'cat': 68,\n",
       " 'help': 69,\n",
       " 'type': 70,\n",
       " 'forgiv': 71,\n",
       " 'error': 72,\n",
       " 'crazi': 73,\n",
       " 'day': 74,\n",
       " 'school': 75,\n",
       " '10': 76,\n",
       " 'hour': 77,\n",
       " 'straiiight': 78,\n",
       " 'watch': 79,\n",
       " 'hill': 80,\n",
       " 'told': 81,\n",
       " 'ha': 82,\n",
       " 'happi': 83,\n",
       " 'birthday': 84,\n",
       " 'jb': 85,\n",
       " 'forget': 86,\n",
       " 'two': 87,\n",
       " 'half': 88,\n",
       " 'men': 89,\n",
       " 'haha': 90,\n",
       " 'worri': 91,\n",
       " 'get': 92,\n",
       " 'hang': 93,\n",
       " 'least': 94,\n",
       " 'feel': 95,\n",
       " 'lost': 96,\n",
       " 'may': 97,\n",
       " 'caus': 98,\n",
       " 'mani': 99,\n",
       " 'later': 100,\n",
       " 'usual': 101,\n",
       " 'night': 102,\n",
       " 'addict': 103,\n",
       " 'sure': 104,\n",
       " 'dm': 105,\n",
       " 'post': 106,\n",
       " 'link': 107,\n",
       " 'video': 108,\n",
       " '<lol>': 109,\n",
       " 'miss': 110,\n",
       " 'better': 111,\n",
       " 'permiss': 112,\n",
       " 'bless': 113,\n",
       " 'first': 114,\n",
       " 'ad': 115,\n",
       " 'tweeti': 116,\n",
       " 'new': 117,\n",
       " 'iphon': 118,\n",
       " 'know': 119,\n",
       " 'think': 120,\n",
       " 'globe': 121,\n",
       " 'yeah': 122,\n",
       " 'sana': 123,\n",
       " 'gumal': 124,\n",
       " 'na': 125,\n",
       " 'ko': 126,\n",
       " 'para': 127,\n",
       " 'alam': 128,\n",
       " 'din': 129,\n",
       " 'kung': 130,\n",
       " 'makakasama': 131,\n",
       " 'ako': 132,\n",
       " 'pictur': 133,\n",
       " 'sweet': 134,\n",
       " 'catch': 135,\n",
       " 'email': 136,\n",
       " 'rss': 137,\n",
       " 'random': 138,\n",
       " 'bacn': 139,\n",
       " \"i'm\": 140,\n",
       " 'cut': 141,\n",
       " 'earli': 142,\n",
       " '11:30': 143,\n",
       " 'pm': 144,\n",
       " 'dinner': 145,\n",
       " 'danc': 146,\n",
       " 'around': 147,\n",
       " 'room': 148,\n",
       " 'pj': 149,\n",
       " 'jam': 150,\n",
       " 'ipod': 151,\n",
       " 'dizzi': 152,\n",
       " 'well': 153,\n",
       " 'ask': 154,\n",
       " 'place': 155,\n",
       " 'peep': 156,\n",
       " 'contest': 157,\n",
       " 'vote': 158,\n",
       " 'anyway': 159,\n",
       " 'go': 160,\n",
       " 'bed': 161,\n",
       " 'goodnight': 162,\n",
       " 'dream': 163,\n",
       " 'walk': 164,\n",
       " 'put': 165,\n",
       " 'deposit': 166,\n",
       " 'tomorrow': 167,\n",
       " 'followinq': 168,\n",
       " 'shud': 169,\n",
       " 'tha': 170,\n",
       " 'meant': 171,\n",
       " 'back': 172,\n",
       " 'toilet': 173,\n",
       " 'cubicl': 174,\n",
       " 'somewher': 175,\n",
       " 'aw': 176,\n",
       " 'hold': 177,\n",
       " 'puppi': 178,\n",
       " \"he'\": 179,\n",
       " 'mine': 180,\n",
       " 'cuti': 181,\n",
       " 'kitteh': 182,\n",
       " 'sleepin': 183,\n",
       " 'crotch': 184,\n",
       " 'prove': 185,\n",
       " 'agre': 186,\n",
       " 'reach': 187,\n",
       " 'amritsar': 188,\n",
       " 'bu': 189,\n",
       " 'wagah': 190,\n",
       " 'border': 191,\n",
       " '2pm': 192,\n",
       " 'let': 193,\n",
       " 'interview': 194,\n",
       " 'famou': 195,\n",
       " 'super': 196,\n",
       " 'star': 197,\n",
       " 'spend': 198,\n",
       " 'famili': 199,\n",
       " 'tire': 200,\n",
       " 'gonna': 201,\n",
       " 'didnt': 202,\n",
       " '=/': 203,\n",
       " 'ef': 204,\n",
       " 'throat': 205,\n",
       " '...': 206,\n",
       " 'ooohh': 207,\n",
       " 'crave': 208,\n",
       " 'pina': 209,\n",
       " 'colada': 210,\n",
       " 'banana': 211,\n",
       " 'slushi': 212,\n",
       " 'upload': 213,\n",
       " 'di': 214,\n",
       " 'indowebst': 215,\n",
       " 'dong': 216,\n",
       " 'banggg': 217,\n",
       " 'welcom': 218,\n",
       " 'glad': 219,\n",
       " 'enjoy': 220,\n",
       " 'hahaha': 221,\n",
       " 'omg': 222,\n",
       " 'wer': 223,\n",
       " 'laughin': 224,\n",
       " 'hook': 225,\n",
       " 'cuz': 226,\n",
       " 'da': 227,\n",
       " 'rolll': 228,\n",
       " 'ryt': 229,\n",
       " 'sheldawg': 230,\n",
       " 'awww': 231,\n",
       " 'pooki': 232,\n",
       " \"i'll\": 233,\n",
       " 'pray': 234,\n",
       " 'bag': 235,\n",
       " 'nurs': 236,\n",
       " 'yay': 237,\n",
       " 'found': 238,\n",
       " 'cuddl': 239,\n",
       " 'buddi': 240,\n",
       " 'met': 241,\n",
       " 'snob': 242,\n",
       " 'bad': 243,\n",
       " 'life': 244,\n",
       " 'goe': 245,\n",
       " 'across': 246,\n",
       " 'univers': 247,\n",
       " 'sleep': 248,\n",
       " 'rehears': 249,\n",
       " 'dri': 250,\n",
       " 'potato': 251,\n",
       " 'huh': 252,\n",
       " 'hi': 253,\n",
       " 'jon': 254,\n",
       " 'hear': 255,\n",
       " 'see': 256,\n",
       " 'cruis': 257,\n",
       " 'cannot': 258,\n",
       " 'knight': 259,\n",
       " 'long': 260,\n",
       " 'convers': 261,\n",
       " 'mom': 262,\n",
       " 'phone': 263,\n",
       " 'yayyy': 264,\n",
       " 'lol': 265,\n",
       " 'request': 266,\n",
       " 'herrr': 267,\n",
       " 'say': 268,\n",
       " 'practic': 269,\n",
       " 'line': 270,\n",
       " 'manner': 271,\n",
       " 'voic': 272,\n",
       " 'upcom': 273,\n",
       " 'featur': 274,\n",
       " 'shoot': 275,\n",
       " 'probabl': 276,\n",
       " 'drive': 277,\n",
       " 'brock': 278,\n",
       " 'read': 279,\n",
       " 'moon': 280,\n",
       " '3rd': 281,\n",
       " 'enough': 282,\n",
       " 'twilight': 283,\n",
       " 'seri': 284,\n",
       " 'today': 285,\n",
       " 'month': 286,\n",
       " 'anniversari': 287,\n",
       " 'sooo': 288,\n",
       " 'diana': 289,\n",
       " 'dont': 290,\n",
       " 'even': 291,\n",
       " 'tar': 292,\n",
       " 'heel': 293,\n",
       " 'ncaa': 294,\n",
       " 'woot': 295,\n",
       " 'bore': 296,\n",
       " 'wallpap': 297,\n",
       " 'red': 298,\n",
       " 'squar': 299,\n",
       " 'notic': 300,\n",
       " 'bb': 301,\n",
       " 'tryna': 302,\n",
       " 'inspir': 303,\n",
       " 'that': 304,\n",
       " \"can't\": 305,\n",
       " 'workout': 306,\n",
       " 'ur': 307,\n",
       " 'hot': 308,\n",
       " 'ass': 309,\n",
       " 'austin': 310,\n",
       " 'annnd': 311,\n",
       " 'bob': 312,\n",
       " 'purpl': 313,\n",
       " 'went': 314,\n",
       " 'royal': 315,\n",
       " 'color': 316,\n",
       " 'way': 317,\n",
       " 'bunch': 318,\n",
       " 'buffi': 319,\n",
       " 'song': 320,\n",
       " 'fave': 321,\n",
       " 'vivian': 322,\n",
       " 'nerf': 323,\n",
       " 'herder': 324,\n",
       " 'faith': 325,\n",
       " 'spike': 326,\n",
       " \"buffy'\": 327,\n",
       " 'bodi': 328,\n",
       " 'devun': 329,\n",
       " 'check': 330,\n",
       " '-->': 331,\n",
       " 'morn': 332,\n",
       " 'tweetland': 333,\n",
       " 'ahead': 334,\n",
       " 'friendster': 335,\n",
       " 'pic': 336,\n",
       " ';-p': 337,\n",
       " 'noodlebox': 338,\n",
       " 'amanda': 339,\n",
       " 'sorrri': 340,\n",
       " '2982378': 341,\n",
       " '2': 342,\n",
       " 'differ': 343,\n",
       " 'thing': 344,\n",
       " 'kevin': 345,\n",
       " 'jona': 346,\n",
       " 'girlfriend': 347,\n",
       " 'seriou': 348,\n",
       " 'sing': 349,\n",
       " '..': 350,\n",
       " 'whoa': 351,\n",
       " 'hehe': 352,\n",
       " 'lsd': 353,\n",
       " 'mayb': 354,\n",
       " 'j': 355,\n",
       " 'k': 356,\n",
       " 'lolol': 357,\n",
       " 'sicilian': 358,\n",
       " 'damn': 359,\n",
       " 'pizza': 360,\n",
       " 'planet': 361,\n",
       " 'chocol': 362,\n",
       " 'milk': 363,\n",
       " 'gf': 364,\n",
       " 'sutter': 365,\n",
       " 'creek': 366,\n",
       " 'tour': 367,\n",
       " 'old': 368,\n",
       " 'look': 369,\n",
       " 'cho': 370,\n",
       " 'person': 371,\n",
       " 'strawberri': 372,\n",
       " 'becom': 373,\n",
       " 'coach': 374,\n",
       " 'soon': 375,\n",
       " 'guess': 376,\n",
       " 'live': 377,\n",
       " 'side': 378,\n",
       " 'earth': 379,\n",
       " 'east': 380,\n",
       " 'mesa': 381,\n",
       " 'losiento': 382,\n",
       " 'apart': 383,\n",
       " 'sore': 384,\n",
       " 'tryout': 385,\n",
       " 'fail': 386,\n",
       " 'physic': 387,\n",
       " 'test': 388,\n",
       " 'homo': 389,\n",
       " 'keeen': 390,\n",
       " 'holiday': 391,\n",
       " 'observ': 392,\n",
       " 'updat': 393,\n",
       " 'alexa': 394,\n",
       " 'websit': 395,\n",
       " 'us': 396,\n",
       " 'total': 397,\n",
       " 'geek': 398,\n",
       " 'right': 399,\n",
       " 'bonjour': 400,\n",
       " 'spacecowboy': 401,\n",
       " 'wish': 402,\n",
       " 'either': 403,\n",
       " 'work': 404,\n",
       " 'kid': 405,\n",
       " 'whenev': 406,\n",
       " 'want': 407,\n",
       " 'wherev': 408,\n",
       " 'brother': 409,\n",
       " 'quit': 410,\n",
       " 'weird': 411,\n",
       " 'give': 412,\n",
       " 'hug': 413,\n",
       " 'cooki': 414,\n",
       " 'hey': 415,\n",
       " 'take': 416,\n",
       " 'care': 417,\n",
       " 'rundown': 418,\n",
       " 'wrote': 419,\n",
       " 'wall': 420,\n",
       " 'correct': 421,\n",
       " 'poor': 422,\n",
       " 'spell': 423,\n",
       " 'highlight': 424,\n",
       " 'wash': 425,\n",
       " 'hardcor': 426,\n",
       " 'good': 427,\n",
       " 'tech': 428,\n",
       " 'clubzon': 429,\n",
       " 'sushi': 430,\n",
       " 'suggest': 431,\n",
       " 'bro': 432,\n",
       " 'rock': 433,\n",
       " 'depend': 434,\n",
       " 'version': 435,\n",
       " 'thought': 436,\n",
       " ';)': 437,\n",
       " 'hand': 438,\n",
       " 'still': 439,\n",
       " 'pretti': 440,\n",
       " 'weak': 441,\n",
       " 'punch': 442,\n",
       " 'anyon': 443,\n",
       " 'yet': 444,\n",
       " 'turn': 445,\n",
       " 'knob': 446,\n",
       " 'doesnt': 447,\n",
       " 'lurker': 448,\n",
       " 'interest': 449,\n",
       " 'b': 450,\n",
       " 'north': 451,\n",
       " 'need': 452,\n",
       " 'decid': 453,\n",
       " 'aaru': 454,\n",
       " 'hithavaru': 455,\n",
       " 'ninag': 456,\n",
       " 'ee': 457,\n",
       " 'moovarolag': 458,\n",
       " 'um': 459,\n",
       " 'bought': 460,\n",
       " 'shit': 461,\n",
       " 'korean': 462,\n",
       " 'oach': 463,\n",
       " 'highest': 464,\n",
       " 'qualiti': 465,\n",
       " 'babi': 466,\n",
       " 'paid': 467,\n",
       " 'extra': 468,\n",
       " '3.50': 469,\n",
       " 'tag': 470,\n",
       " 'sew': 471,\n",
       " 'ice-cream': 472,\n",
       " 'top': 473,\n",
       " 'finish': 474,\n",
       " '200': 475,\n",
       " 'crunch': 476,\n",
       " 'step': 477,\n",
       " 'diddi': 478,\n",
       " 'daaang': 479,\n",
       " 'possibl': 480,\n",
       " \"what'd\": 481,\n",
       " 'west': 482,\n",
       " 'mostli': 483,\n",
       " 'san': 484,\n",
       " 'jose': 485,\n",
       " 'francisco': 486,\n",
       " 'nikki': 487,\n",
       " 'bike': 488,\n",
       " 'tri': 489,\n",
       " 'recov': 490,\n",
       " 'knee': 491,\n",
       " 'injuri': 492,\n",
       " 'would': 493,\n",
       " 'unpack': 494,\n",
       " 'sat': 495,\n",
       " 'afternoon': 496,\n",
       " 'shenagian': 497,\n",
       " 'team': 498,\n",
       " 'awesom': 499,\n",
       " '<3': 500,\n",
       " 'listen': 501,\n",
       " 'music': 502,\n",
       " 'chill': 503,\n",
       " 'regret': 504,\n",
       " 'done': 505,\n",
       " 'till': 506,\n",
       " 'kick': 507,\n",
       " 'studi': 508,\n",
       " 'grow': 509,\n",
       " 'rich': 510,\n",
       " 'wow': 511,\n",
       " 'amaz': 512,\n",
       " 'timeless': 513,\n",
       " 'law': 514,\n",
       " 'attract': 515,\n",
       " \"we'r\": 516,\n",
       " '6': 517,\n",
       " 'program': 518,\n",
       " 'oh': 519,\n",
       " \"i'v\": 520,\n",
       " \"it'll\": 521,\n",
       " 'week': 522,\n",
       " 'though': 523,\n",
       " '21': 524,\n",
       " 'chri': 525,\n",
       " 'home': 526,\n",
       " '67': 527,\n",
       " 'summer': 528,\n",
       " '1.47': 529,\n",
       " 'ish': 530,\n",
       " 'tweet': 531,\n",
       " 'drink': 532,\n",
       " 'kay': 533,\n",
       " 'determin': 534,\n",
       " 'basebal': 535,\n",
       " '5': 536,\n",
       " 'year': 537,\n",
       " 'sinc': 538,\n",
       " 'sport': 539,\n",
       " 'game': 540,\n",
       " 'becca': 541,\n",
       " 'ohhh': 542,\n",
       " 'nkow': 543,\n",
       " 'lexington': 544,\n",
       " 'ky': 545,\n",
       " 'rite': 546,\n",
       " 'next': 547,\n",
       " 'seen': 548,\n",
       " 'preview': 549,\n",
       " 'movi': 550,\n",
       " 'obsess': 551,\n",
       " 'w': 552,\n",
       " 'beyonc': 553,\n",
       " 'broad': 554,\n",
       " 'seem': 555,\n",
       " 'nut': 556,\n",
       " 'easi': 557,\n",
       " 'rider': 558,\n",
       " 'fall': 559,\n",
       " 'asleep': 560,\n",
       " 'excit': 561,\n",
       " 'broadcast': 562,\n",
       " 'paris-roubaix': 563,\n",
       " 'moment': 564,\n",
       " 'misss': 565,\n",
       " 'peopl': 566,\n",
       " 'surviv': 567,\n",
       " 'winter': 568,\n",
       " 'without': 569,\n",
       " 'jungl': 570,\n",
       " 'gym': 571,\n",
       " 'start': 572,\n",
       " 'last': 573,\n",
       " 'holidayholiday': 574,\n",
       " 'watchin': 575,\n",
       " 'greek': 576,\n",
       " 'knighti': 577,\n",
       " 'media': 578,\n",
       " 'design': 579,\n",
       " 'client': 580,\n",
       " 'profil': 581,\n",
       " 'lame': 582,\n",
       " 'stalk': 583,\n",
       " 'sexi': 584,\n",
       " 'aww': 585,\n",
       " 'anoth': 586,\n",
       " 'nitey': 587,\n",
       " 'nite': 588,\n",
       " 'ahh': 589,\n",
       " 'kinda': 590,\n",
       " 'nervou': 591,\n",
       " 'til': 592,\n",
       " 'plan': 593,\n",
       " 'whatev': 594,\n",
       " 'happier': 595,\n",
       " \"here'\": 596,\n",
       " 'hopin': 597,\n",
       " 'fb': 598,\n",
       " 'store': 599,\n",
       " 'mean': 600,\n",
       " 'pam': 601,\n",
       " 'ye': 602,\n",
       " 'short': 603,\n",
       " 'struggl': 604,\n",
       " 'choic': 605,\n",
       " 'everi': 606,\n",
       " 'hmm': 607,\n",
       " 'lot': 608,\n",
       " \"that'\": 609,\n",
       " 'cours': 610,\n",
       " 'late': 611,\n",
       " '100,000': 612,\n",
       " 'follow': 613,\n",
       " 'almost': 614,\n",
       " '4': 615,\n",
       " 'nm': 616,\n",
       " 'scream': 617,\n",
       " 'wonder': 618,\n",
       " 'freakin': 619,\n",
       " 'coool': 620,\n",
       " \"there'\": 621,\n",
       " 'noth': 622,\n",
       " 'wrong': 623,\n",
       " 'brissa': 624,\n",
       " 'lauren': 625,\n",
       " 'stori': 626,\n",
       " 'mirror': 627,\n",
       " 'neat': 628,\n",
       " 'god': 629,\n",
       " 'overitnow': 630,\n",
       " 'apolog': 631,\n",
       " 'share': 632,\n",
       " 'spiritu': 633,\n",
       " 'gener': 634,\n",
       " 'public': 635,\n",
       " '31': 636,\n",
       " 'leav': 637,\n",
       " 'ontario': 638,\n",
       " 'spoke': 639,\n",
       " 'fam': 640,\n",
       " 'japan': 641,\n",
       " 'via': 642,\n",
       " 'skype': 643,\n",
       " 'littl': 644,\n",
       " '16': 645,\n",
       " 'mo-old': 646,\n",
       " 'nephew': 647,\n",
       " 'saw': 648,\n",
       " 'written': 649,\n",
       " 'someon': 650,\n",
       " 'spoken': 651,\n",
       " 'suck': 652,\n",
       " 'mcfli': 653,\n",
       " 'loser': 654,\n",
       " 'holi': 655,\n",
       " 'laundri': 656,\n",
       " 'woman': 657,\n",
       " 'hate': 658,\n",
       " 'trainer': 659,\n",
       " 'youï': 660,\n",
       " '¿': 661,\n",
       " '½': 662,\n",
       " 'fuck': 663,\n",
       " 'funni': 664,\n",
       " 'set': 665,\n",
       " 'touch': 666,\n",
       " 'offici': 667,\n",
       " 'tvc': 668,\n",
       " 'yesterday': 669,\n",
       " 'could': 670,\n",
       " 'manni': 671,\n",
       " 'peddi': 672,\n",
       " 'shut': 673,\n",
       " 'fat': 674,\n",
       " 'face': 675,\n",
       " 'ever': 676,\n",
       " 'run': 677,\n",
       " 'idea': 678,\n",
       " 'nomin': 679,\n",
       " 'beer': 680,\n",
       " 'monday': 681,\n",
       " 'never': 682,\n",
       " 'alway': 683,\n",
       " 'special': 684,\n",
       " '72': 685,\n",
       " 'luck': 686,\n",
       " 'rest': 687,\n",
       " 'cast': 688,\n",
       " 'tb': 689,\n",
       " 'someth': 690,\n",
       " 'leak': 691,\n",
       " 'land': 692,\n",
       " 'among': 693,\n",
       " 'entir': 694,\n",
       " 'creativ': 695,\n",
       " 'vibrant': 696,\n",
       " 'reason': 697,\n",
       " 'nada': 698,\n",
       " 'holla': 699,\n",
       " 'how': 700,\n",
       " 'mixtap': 701,\n",
       " 'yep': 702,\n",
       " 'easier': 703,\n",
       " 'point': 704,\n",
       " '140': 705,\n",
       " 'freezer-worthi': 706,\n",
       " 'given': 707,\n",
       " 'scholarli': 708,\n",
       " 'attent': 709,\n",
       " 'suppos': 710,\n",
       " 'eye': 711,\n",
       " 'favor': 712,\n",
       " 'use': 713,\n",
       " 'english': 714,\n",
       " 'languag': 715,\n",
       " 'explet': 716,\n",
       " 'flippin': 717,\n",
       " 'cold': 718,\n",
       " 'outsid': 719,\n",
       " 'ago': 720,\n",
       " 'sunni': 721,\n",
       " 'rain': 722,\n",
       " 'snow': 723,\n",
       " 'ate': 724,\n",
       " 'raw': 725,\n",
       " 'dough': 726,\n",
       " 'tub': 727,\n",
       " 'must': 728,\n",
       " 'kindr': 729,\n",
       " 'spirit': 730,\n",
       " 'internet': 731,\n",
       " 'dirti': 732,\n",
       " 'nighti': 733,\n",
       " 'tweetdeck': 734,\n",
       " 'facebook': 735,\n",
       " 'dude': 736,\n",
       " 'surpris': 737,\n",
       " 'figur': 738,\n",
       " 'llama': 739,\n",
       " 'malcolm': 740,\n",
       " 'megan': 741,\n",
       " '<---': 742,\n",
       " 'theatr': 743,\n",
       " 'martini': 744,\n",
       " 'citi': 745,\n",
       " 'soo': 746,\n",
       " 'spring': 747,\n",
       " 'break': 748,\n",
       " 'moviess': 749,\n",
       " 'everyday': 750,\n",
       " 'grandpa': 751,\n",
       " 'ice': 752,\n",
       " 'chest': 753,\n",
       " 'beef': 754,\n",
       " 'georg': 755,\n",
       " 'lopez': 756,\n",
       " 'rulesss': 757,\n",
       " 'kati': 758,\n",
       " 'head': 759,\n",
       " 'club': 760,\n",
       " 'speak': 761,\n",
       " 'stellar': 762,\n",
       " 'homey': 763,\n",
       " 'shhh': 764,\n",
       " 'photo': 765,\n",
       " 'hit': 766,\n",
       " 'count': 767,\n",
       " 'china': 768,\n",
       " '5,000': 769,\n",
       " 'view': 770,\n",
       " 'alon': 771,\n",
       " 'came': 772,\n",
       " 'appreci': 773,\n",
       " 'everywher': 774,\n",
       " 'provid': 775,\n",
       " 'broadband': 776,\n",
       " 'kiddin': 777,\n",
       " 'callin': 778,\n",
       " 'hash': 779,\n",
       " 'bash': 780,\n",
       " 'badass': 781,\n",
       " 'fring': 782,\n",
       " 'hmmm': 783,\n",
       " 'upto': 784,\n",
       " 'christma': 785,\n",
       " 'bn': 786,\n",
       " 'fun': 787,\n",
       " 'caught': 788,\n",
       " 'yrold': 789,\n",
       " 'cheat': 790,\n",
       " 'reflect': 791,\n",
       " 'tv': 792,\n",
       " 'hair': 793,\n",
       " 'blue': 794,\n",
       " 'kind': 795,\n",
       " 'word': 796,\n",
       " 'stress': 797,\n",
       " 'math': 798,\n",
       " 'uhhh': 799,\n",
       " 'uh': 800,\n",
       " 'driver': 801,\n",
       " 'pch': 802,\n",
       " 'whatcha': 803,\n",
       " 'chucklin': 804,\n",
       " 'bout': 805,\n",
       " 'ms.yen': 806,\n",
       " 'climb': 807,\n",
       " 'sine': 808,\n",
       " '20': 809,\n",
       " 'unc': 810,\n",
       " 'prepar': 811,\n",
       " \"g'night\": 812,\n",
       " 'includ': 813,\n",
       " 'magic': 814,\n",
       " 'fountain': 815,\n",
       " 'trip': 816,\n",
       " 'trusti': 817,\n",
       " 'book': 818,\n",
       " 'anastasia': 819,\n",
       " 'laptop': 820,\n",
       " 'buzz': 821,\n",
       " 'sietar': 822,\n",
       " 'push': 823,\n",
       " 'boundari': 824,\n",
       " 'boy': 825,\n",
       " 'money': 826,\n",
       " 'manual': 827,\n",
       " 'terribl': 828,\n",
       " 'neck': 829,\n",
       " 'ach': 830,\n",
       " 'chair': 831,\n",
       " 'swopper': 832,\n",
       " 'arriv': 833,\n",
       " 'hellz': 834,\n",
       " 'definit': 835,\n",
       " 'download': 836,\n",
       " 'hahah': 837,\n",
       " 'blog': 838,\n",
       " 'nice': 839,\n",
       " 'recent': 840,\n",
       " 'popular': 841,\n",
       " 'list': 842,\n",
       " 'togeth': 843,\n",
       " 'common': 844,\n",
       " 'wanna': 845,\n",
       " 'save': 846,\n",
       " 'dog': 847,\n",
       " 'anim': 848,\n",
       " 'call': 849,\n",
       " 'twit': 850,\n",
       " 'polic': 851,\n",
       " 'nilagyan': 852,\n",
       " 'mo': 853,\n",
       " 'ng': 854,\n",
       " 'knorr': 855,\n",
       " 'season': 856,\n",
       " 'mix': 857,\n",
       " 'ulit': 858,\n",
       " 'ung': 859,\n",
       " 'pancak': 860,\n",
       " 'heheh': 861,\n",
       " 'teka': 862,\n",
       " 'heaven': 863,\n",
       " 'egg': 864,\n",
       " 'ba': 865,\n",
       " 'hello': 866,\n",
       " 'westney': 867,\n",
       " 'x': 868,\n",
       " 'comparison': 869,\n",
       " 'stroke': 870,\n",
       " 'experienc': 871,\n",
       " 'draw': 872,\n",
       " 'giraff': 873,\n",
       " 'eyelash': 874,\n",
       " 'ahhh': 875,\n",
       " 'shiner': 876,\n",
       " 'bock': 877,\n",
       " 'reliv': 878,\n",
       " 'spent': 879,\n",
       " 'beauti': 880,\n",
       " 'warm': 881,\n",
       " 'weather': 882,\n",
       " 'pullin': 883,\n",
       " 'nighter': 884,\n",
       " 'wit': 885,\n",
       " 'bffl': 886,\n",
       " 'madison': 887,\n",
       " 'event': 888,\n",
       " 'inevit': 889,\n",
       " 'sequel': 890,\n",
       " 'hilari': 891,\n",
       " 'row': 892,\n",
       " 'dooo': 893,\n",
       " 'big': 894,\n",
       " 'oaf': 895,\n",
       " 'play': 896,\n",
       " 'piano': 897,\n",
       " 'bryon': 898,\n",
       " 'tea': 899,\n",
       " 'bitch': 900,\n",
       " 'stay': 901,\n",
       " 'yup': 902,\n",
       " 'match': 903,\n",
       " 'offic': 904,\n",
       " 'harder': 905,\n",
       " 'peac': 906,\n",
       " 'prayer': 907,\n",
       " 'ya': 908,\n",
       " 'beat': 909,\n",
       " 'traffic': 910,\n",
       " 'india': 911,\n",
       " 'win': 912,\n",
       " 'zealand': 913,\n",
       " '41': 914,\n",
       " 'wasnt': 915,\n",
       " 'third': 916,\n",
       " 'madra': 917,\n",
       " 'road': 918,\n",
       " 'concept': 919,\n",
       " 'park': 920,\n",
       " 'appli': 921,\n",
       " 'class': 922,\n",
       " 'manga': 923,\n",
       " 'bring': 924,\n",
       " 'joy': 925,\n",
       " 'roflmao': 926,\n",
       " 'niqqa': 927,\n",
       " 'pleas': 928,\n",
       " 'ackin': 929,\n",
       " 'fool': 930,\n",
       " 'also': 931,\n",
       " 'fyi': 932,\n",
       " 'piti': 933,\n",
       " 'knew': 934,\n",
       " \"let'\": 935,\n",
       " 'profession': 936,\n",
       " 'octob': 937,\n",
       " '2008': 938,\n",
       " \"we'v\": 939,\n",
       " 'anyth': 940,\n",
       " 'tip': 941,\n",
       " \"sam'\": 942,\n",
       " 'nighttim': 943,\n",
       " 'twin': 944,\n",
       " 'ember': 945,\n",
       " 'xd': 946,\n",
       " 'die': 947,\n",
       " 'actual': 948,\n",
       " 'keep': 949,\n",
       " ':d': 950,\n",
       " 'rive': 951,\n",
       " 'mate': 952,\n",
       " 'darn': 953,\n",
       " \"neil'\": 954,\n",
       " 'nick': 955,\n",
       " \"nora'\": 956,\n",
       " 'playlist': 957,\n",
       " 'perfect': 958,\n",
       " 'venu': 959,\n",
       " 'whine': 960,\n",
       " 'wast': 961,\n",
       " 'hell': 962,\n",
       " 'effin': 963,\n",
       " 'man': 964,\n",
       " 'lazi': 965,\n",
       " 'tool': 966,\n",
       " 'messag': 967,\n",
       " 'ai': 968,\n",
       " 'buy': 969,\n",
       " 'tina': 970,\n",
       " \"parol'\": 971,\n",
       " \"who'\": 972,\n",
       " 'itun': 973,\n",
       " 'debbi': 974,\n",
       " 'sixti': 975,\n",
       " 'six': 976,\n",
       " 'bowl': 977,\n",
       " 'dear': 978,\n",
       " 'dbq': 979,\n",
       " 'pain': 980,\n",
       " 'asss': 981,\n",
       " 'chat': 982,\n",
       " 'coincid': 983,\n",
       " 'feed': 984,\n",
       " 'gsoc': 985,\n",
       " 'news': 986,\n",
       " 'discov': 987,\n",
       " 'easter': 988,\n",
       " 'craft': 989,\n",
       " 'cross': 990,\n",
       " 'bun': 991,\n",
       " 'princ': 992,\n",
       " 'egypt': 993,\n",
       " 'aid': 994,\n",
       " 'medit': 995,\n",
       " 'nap': 996,\n",
       " 'twitterpeep': 997,\n",
       " 'pay': 998,\n",
       " 'forward': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder_path = Path(\"data/raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0c3c0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "List = open(val_pos_path, encoding = 'utf-8').readlines()\n",
    "for i in range(len(List)):\n",
    "    List[i] = List[i].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2283cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "List = open(y_valid_path, encoding = 'utf-8').readlines()\n",
    "for i in range(len(List)):\n",
    "    List[i] = float(List[i])\n",
    "List = np.array(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "945806a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List == val_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79147323",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(vocab_path, 'r', encoding = 'utf-8')\n",
    "\n",
    "jsondata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c193542b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondata == Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b53d3d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_path = prepared_folder_path / \"val_pos.txt\"\n",
    "val_neg_path = prepared_folder_path / \"val_neg.txt\"\n",
    "\n",
    "with open(val_pos_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in val_pos:\n",
    "        temp_file.write(\"%s\\n\" % item)\n",
    "        \n",
    "with open(val_neg_path, 'w', encoding = 'utf-8') as temp_file:\n",
    "    for item in val_neg:\n",
    "        temp_file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a0b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
